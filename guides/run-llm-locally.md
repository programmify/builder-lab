# Run LLMs Locally (Ollama / Local models)

1. Install Ollama (or preferred local LLM runtime).
2. Pull a model and start the Ollama server.
3. Connect from your app using the local endpoint.
4. For privacy, ensure models and data stay on the device when required.
